{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# 1. Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 2. Normalize the data (scale pixel values to range [0, 1])\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 3. Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 4. Split the training data into training and validation sets (80% train, 20% validation)\n",
    "validation_split = 0.2\n",
    "num_train_samples = int((1 - validation_split) * x_train.shape[0])\n",
    "\n",
    "x_train_final = x_train[:num_train_samples]\n",
    "y_train_final = y_train[:num_train_samples]\n",
    "\n",
    "x_val = x_train[num_train_samples:]\n",
    "y_val = y_train[num_train_samples:]\n",
    "\n",
    "# 5. Define image size for VGG16\n",
    "IMG_SIZE = 224  # VGG16 expects 224x224 images\n",
    "\n",
    "# 6. Function to preprocess and resize images\n",
    "def preprocess_image(image, label):\n",
    "    # Resize the image to the desired size\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "# 7. Create tf.data.Dataset objects for training, validation, and testing\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "\n",
    "# Training Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_final, y_train_final))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000)  # Shuffle the dataset\n",
    "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)  # Resize images\n",
    "train_dataset = train_dataset.batch(batch_size)  # Batch the data\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch for performance\n",
    "\n",
    "# Validation Dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Test Dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 8. Load the pre-trained VGG16 feature extractor from Keras Applications\n",
    "vgg16_base = VGG16(\n",
    "    include_top=False,  # Exclude the top classification layers\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Freeze the VGG16 base to prevent its weights from being updated during training\n",
    "vgg16_base.trainable = False\n",
    "\n",
    "# 9. Build the model using the Keras Functional API\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"images\")\n",
    "x = vgg16_base(inputs, training=False)  # Set training=False to avoid updating batch normalization layers\n",
    "x = layers.Flatten()(x)  # Flatten the output of VGG16\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 10. Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 11. Train the model using the tf.data.Dataset objects\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset\n",
    ")\n",
    "\n",
    "# 12. Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# 13. Generate predictions on a few test samples\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "for images, labels in test_dataset.take(1):\n",
    "    predictions = model.predict(images[:3])\n",
    "    print(\"Predictions shape:\", predictions.shape)\n",
    "    print(predictions)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
