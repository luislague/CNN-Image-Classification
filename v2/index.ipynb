{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with CNN\n",
    "\n",
    "In this project we will build a convolutional neural network (CNN) to classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "# Image Dataset\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 colour images in 10 categories - airplanes, dogs, cats, and other objects. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "In the following we will preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  Next we will build a convolutional, max pooling, dropout, and fully connected layers. At the end, we will train the network ang get to see it's predictions on the sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "In this section, we will load the CIFAR-10 dataset, normalize the pixel values, and prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Load and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and split between testing and training data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Checking the Size of the Data\n",
    "\n",
    "\n",
    "We'll now check the shapes of the training and testing data to understand the dataset structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes of the dataset\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Normalizing the Data\n",
    "\n",
    "We will normalize the pixel values of the images by scaling them between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Data augmentation: starting with a small rotation range between the standard so the images don't get distorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Defining the CNN Model\n",
    "\n",
    "\n",
    "We will define a CNN model with three convolutional layers followed by max-pooling and dense layers for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 F1-Score Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom F1-Score metric as a class\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Update the precision and recall\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        # Compute the F1 score using the precision and recall\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Model Parameters\n",
    "\n",
    "\n",
    "Here, we set key parameters for the model such as the input shape and the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "# Define the VGG-style model\n",
    "  model = Sequential([\n",
    "    # Block 1\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "    Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # Block 2\n",
    "    Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # Block 3\n",
    "    Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # Block 4\n",
    "    Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # Block 5\n",
    "    Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dropout(0.5),  # Dropout to reduce overfitting\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n",
    "    Dense(num_classes, activation='softmax')  # Final classification layer\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = create_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Model summary \n",
    "\n",
    "print out the model summary to visualize the architecture, the number of layers, and the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Learning Rate Scheduler\n",
    "\n",
    "\n",
    "We will define a custom learning rate scheduler that reduces the learning rate by 10% every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = lr * 0.9\n",
    "    return lr\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "lr_scheduler = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Model Compliation\n",
    "\n",
    "\n",
    "We will compile the model using Adam optimizer and categorical cross-entropy as the loss function, tracking accuracy as a metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = F1Score()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall(), f1_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training\n",
    "\n",
    "We will now train the model for 25 epochs with a batch size of 512, using the learning rate scheduler callback to adjust the learning rate dynamically during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=512, \n",
    "                    epochs=25, \n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Evaluation\n",
    "We will evaluate the model on the test data and calculate the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {test_results[0]}')\n",
    "print(f'Test Accuracy: {test_results[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Bar Plot for Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, train_precision, train_recall, train_f1 = model.evaluate(x_train, y_train)\n",
    "\n",
    "print('Train loss:', train_loss)\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Train precision:', train_precision)\n",
    "print('Train recall:', train_recall)\n",
    "print('Train F1-score:', train_f1)\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall, test_f1 = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test precision:', test_precision)\n",
    "print('Test recall:', test_recall)\n",
    "print('Test F1-score:', test_f1)\n",
    "\n",
    "# Bar Plot for Training Data Metrics\n",
    "train_metrics = ['Loss', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "train_values = [train_loss, train_acc, train_precision, train_recall, train_f1]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(train_metrics, train_values, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "plt.ylim(0, 1)  # Set y-axis limit to [0, 1] for better visibility\n",
    "plt.title('Model Evaluation Metrics on Training Data')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Metrics')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Subplots for Training History Metrics\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figure size\n",
    "\n",
    "# Cross Entropy Loss\n",
    "plt.subplot(411)  # First subplot (Cross Entropy Loss)\n",
    "plt.plot(history.history['loss'], color='blue', label='Train Loss')\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(412)  # Second subplot (Accuracy)\n",
    "plt.plot(history.history['accuracy'], color='green', label='Train Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Precision and Recall\n",
    "plt.subplot(413)  # Third subplot (Precision)\n",
    "plt.plot(history.history['precision'], color='orange', label='Precision')\n",
    "plt.plot(history.history['recall'], color='red', label='Recall')\n",
    "plt.title('Precision and Recall')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# F1 Score\n",
    "plt.subplot(414)  # Fourth subplot (F1 Score)\n",
    "plt.plot(history.history['f1_score'], color='purple', label='F1-Score')\n",
    "plt.title('F1-Score')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Avoids overlap between subplots\n",
    "print(\"Training Data Metrics:\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "metrics = ['Loss', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [test_loss, test_acc, test_precision, test_recall, test_f1]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(metrics, values, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "plt.ylim(0, 1)  # Set y-axis limit to [0, 1] for better visibility\n",
    "plt.title('Model Evaluation Metrics on Test Data')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Metrics')\n",
    "plt.grid(axis='y')\n",
    "print(\"Test Data Metrics:\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Subplots for Training History Metrics\n",
    "plt.figure(figsize=(12, 12))  # Adjust the figure size\n",
    "\n",
    "# Cross Entropy Loss\n",
    "plt.subplot(411)  # First subplot (Cross Entropy Loss)\n",
    "plt.plot(history.history['loss'], color='blue', label='Train Loss')\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(412)  # Second subplot (Accuracy)\n",
    "plt.plot(history.history['accuracy'], color='green', label='Train Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Precision and Recall\n",
    "plt.subplot(413)  # Third subplot (Precision)\n",
    "plt.plot(history.history['precision'], color='orange', label='Precision')\n",
    "plt.plot(history.history['recall'], color='red', label='Recall')\n",
    "plt.title('Precision and Recall')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# F1 Score\n",
    "plt.subplot(414)  # Fourth subplot (F1 Score)\n",
    "plt.plot(history.history['f1_score'], color='purple', label='F1-Score')\n",
    "plt.title('F1-Score')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Avoids overlap between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Generate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "In this section, we will apply transfer learning using a pre-trained model ResNet50 and fine-tune it for our classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Set the desired image size for ResNet50 (224x224)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Function to resize images\n",
    "def resize_images(image, label):\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    return image, label\n",
    "\n",
    "# Create a tf.data dataset and resize images dynamically\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(resize_images).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(resize_images).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Load ResNet50 feature extractor from TensorFlow Hub\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url, input_shape=(IMG_SIZE, IMG_SIZE, 3), trainable=False)\n",
    "\n",
    "# Build your transfer learning model\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model on CIFAR-10\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=10)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
